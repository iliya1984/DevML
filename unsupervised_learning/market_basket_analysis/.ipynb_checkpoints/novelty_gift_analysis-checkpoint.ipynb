{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe7f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from itertools import permutations\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d4f92f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 172)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:172\u001b[1;36m\u001b[0m\n\u001b[1;33m    if(not (self.filter is None)):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Columns:\n",
    "    INVOICE_NO = 'invoice_no'\n",
    "    STOCK_CODE = 'stock_code'\n",
    "    DESCRIPTION = 'description'\n",
    "\n",
    "class MetricType:\n",
    "    SUPPORT = 'support'\n",
    "    CONFIDENCE = 'confidence'\n",
    "    LIFT = 'lift'\n",
    "\n",
    "class RunResults(list):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        self.statistics = Statistics()\n",
    "        self.items = []\n",
    "            \n",
    "    \n",
    "class SingleRunResults:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.frequent_itemsets = None\n",
    "        self.rules = None\n",
    "\n",
    "class StatisticRecord:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.apriori_min_support = -1\n",
    "        self.apriori_max_len = -1\n",
    "        self.rule_metric = ''\n",
    "        self.rule_min_threshold = -1\n",
    "        self.max_antecedent_support  = -1\n",
    "        self.min_consequent_support = -1\n",
    "        self.zhang = -1\n",
    "        self.frequent_datasets = -1\n",
    "        self.rules = -1\n",
    "        self.configuration_id = ''\n",
    "        \n",
    "class Statistics:\n",
    "    \n",
    "    _column_names = ['apr_min_support', 'apr_max_len', 'rule_metric', \\\n",
    "                     'rule_min_threshold', 'max_support_A', \\\n",
    "                     'min_support_C', 'max_zhang', 'freq_ds', 'rules', 'configuration_id']\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self._df = pd.DataFrame(columns=Statistics._column_names)\n",
    "        self._df.set_index('configuration_id')\n",
    "    \n",
    "    def add_statistic(self, record):\n",
    "        \n",
    "        try:\n",
    "            values = [record.apriori_min_support, record.apriori_max_len, record.rule_metric, \\\n",
    "                  record.rule_min_threshold, record.max_antecedent_support, \\\n",
    "                  record.min_consequent_support, record.zhang, record.frequent_datasets, \\\n",
    "                  record.rules, record.configuration_id]\n",
    "        \n",
    "            self._df.loc[len(self._df.index)] = values\n",
    "        except Exception as ex:\n",
    "            print(f'Unable to add a statistic {values}')\n",
    "            print(f'Error args are {ex.args}\\n')\n",
    "    \n",
    "    def get(self):\n",
    "        return self._df\n",
    "    \n",
    "class Apriori:\n",
    "    \n",
    "    def __init__(self, configuration, df):\n",
    "        self.configuration = configuration\n",
    "        self.df = df\n",
    "       \n",
    "    def zhangs_rule(self, rules):\n",
    "        PAB = rules['support'].copy()\n",
    "        PA = rules['antecedent support'].copy()\n",
    "        PB = rules['consequent support'].copy()\n",
    "        NUMERATOR = PAB - PA*PB\n",
    "        DENOMINATOR = np.max((PAB*(1-PA).values,PA*(PB-PAB).values), axis = 0)\n",
    "        return NUMERATOR / DENOMINATOR\n",
    "    \n",
    "    def run(self):\n",
    "        results = RunResults() \n",
    "        \n",
    "        for option in self.configuration.options:\n",
    "            run_results = SingleRunResults()\n",
    "            \n",
    "            apriori_min_support = option.min_support\n",
    "            apriori_max_len = option.max_len\n",
    "            \n",
    "            run_results.frequent_itemsets = apriori(self.df, \\\n",
    "                min_support = apriori_min_support, \\\n",
    "                max_len = apriori_max_len, use_colnames = True)\n",
    "            \n",
    "            rule_metric = option.association_rule.metric\n",
    "            rule_min_threshold = option.association_rule.min_threshold\n",
    "            \n",
    "            rules =  association_rules( \\\n",
    "                run_results.frequent_itemsets, \\\n",
    "                metric = rule_metric, \\\n",
    "                min_threshold = rule_min_threshold)\n",
    "        \n",
    "            max_antecedent_support = rules['antecedent support'].max()\n",
    "            min_consequent_support = rules['consequent support'].min()\n",
    "            rules['zhang'] = self.zhangs_rule(rules)\n",
    "        \n",
    "            if not (option.filter is None):\n",
    "                \n",
    "                if not (option.filter.antecedent_support is None):\n",
    "                    rules = rules[rules['antecedent support'] > option.filter.antecedent_support]\n",
    "                    \n",
    "                if not (option.filter.consequent_support is None):\n",
    "                    rules = rules[rules['consequent support'] <  option.filter.consequent_support]\n",
    "                \n",
    "                if not (option.filter.zhang is None):\n",
    "                    rules = rules[rules['zhang'] > option.filter.zhang]\n",
    "                \n",
    "            run_results.rules = rules\n",
    "            results.items.append(run_results)\n",
    "            \n",
    "            statistic = StatisticRecord()\n",
    "            statistic.apriori_min_support = apriori_min_support\n",
    "            statistic.apriori_max_len = apriori_max_len\n",
    "            statistic.rule_metric = rule_metric\n",
    "            statistic.rule_min_threshold = rule_min_threshold\n",
    "            statistic.max_antecedent_support  = max_antecedent_support\n",
    "            statistic.min_consequent_support = min_consequent_support\n",
    "            statistic.zhang = rules['zhang'].max()\n",
    "            statistic.frequent_datasets = len(run_results.frequent_itemsets)\n",
    "            statistic.rules = len(run_results.rules)\n",
    "            statistic.configuration_id = option.id\n",
    "            \n",
    "            results.statistics.add_statistic(statistic)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "class AprioriConfiguration:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.options = []\n",
    "        \n",
    "    def get_option(self, option_id):\n",
    "        \n",
    "        for o in self.options:\n",
    "            if o.id == option_id:\n",
    "                return o\n",
    "        return None\n",
    "    \n",
    "class AprioriOption:\n",
    "    _id_counter = 0\n",
    "    \n",
    "    def __init__(self, min_support, max_len):\n",
    "        self.min_support = min_support\n",
    "        self.max_len = max_len\n",
    "        self.association_rule = None\n",
    "        self.filter = None\n",
    "        self.id = AprioriOption.next_id()\n",
    "    \n",
    "    def next_id():\n",
    "        next_id = AprioriOption._id_counter + 1\n",
    "        AprioriOption._id_counter = next_id\n",
    "        return next_id\n",
    "    \n",
    "    def info(self):\n",
    "        print(\"Configuration Info\\n\")\n",
    "        print(f\"id = {self.id}\\n\")\n",
    "        print(f\"min_support = {self.min_support}\\n\")\n",
    "        print(f\"max_len = {self.max_len}\\n\")\n",
    "        \n",
    "        if(not (self.association_rule is None)):\n",
    "            print(f\"association_rule_metric = {self.association_rule.metric}\\n\")\n",
    "            print(f\"association_rule_min_threshold = {self.association_rule.min_threshold}\\n\")\n",
    "        \n",
    "         if(not (self.filter is None)):\n",
    "            print(f\"filter_antecedent_support = {self.filter.antecedent_support}\\n\")\n",
    "            print(f\"filter_consequent_support = {self.filter.consequent_support}\\n\")\n",
    "            print(f\"filter_zhang = {self.filter.zhang}\\n\")\n",
    "    \n",
    "class FilterOption:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.antecedent_support = None\n",
    "        self.consequent_support = None\n",
    "        self.zhang = None\n",
    "        \n",
    "class RAOption:\n",
    "    \n",
    "    def __init__(self, metric, min_threshold):\n",
    "        self.metric = metric\n",
    "        self.min_threshold = min_threshold\n",
    "    \n",
    "        \n",
    "class Aggregate:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    #Select the column headers for sign items\n",
    "    def apply(self, item):\n",
    "        headers = []\n",
    "        \n",
    "        for i in self.df.columns:\n",
    "            word_list = self.__word_list(str(i).lower())\n",
    "            if item in word_list:\n",
    "                headers.append(i)\n",
    "            \n",
    "            \n",
    "        # Select columns for this items\n",
    "        item_columns = self.df[headers]\n",
    "            \n",
    "        # Return category of aggregated items\n",
    "        return item_columns.sum(axis = 1) >= 1.0\n",
    "            \n",
    "    def __word_list(self, value):\n",
    "        splited = list(value.split(' '))\n",
    "        return splited\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gifts_df = pd.read_csv('../datasets/market_basket/online_retail.csv')\n",
    "\n",
    "gifts_df.rename(columns={ \\\n",
    "    'InvoiceNo' : 'invoice_no', \\\n",
    "    'StockCode' : 'stock_code', \\\n",
    "    'Description' : 'description' \\\n",
    "    }, inplace=True)\n",
    "\n",
    "gifts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gifts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove leading and trailing characters in the Description column\n",
    "gifts_df[Columns.DESCRIPTION] = gifts_df[Columns.DESCRIPTION].str.strip()\n",
    "gifts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the rows without any invoice number\n",
    "row_count = len(gifts_df)\n",
    "\n",
    "gifts_df.dropna(subset=[Columns.INVOICE_NO], inplace=True)\n",
    "gifts_df[Columns.INVOICE_NO] = gifts_df[Columns.INVOICE_NO].astype('str')\n",
    "\n",
    "print(f'Row count dropped from {row_count} to {len(gifts_df)}')\n",
    "\n",
    "gifts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all transactions which were done on credit\n",
    "row_count = len(gifts_df)\n",
    "\n",
    "filt = ~gifts_df[Columns.INVOICE_NO].str.contains('C')\n",
    "gifts_df = gifts_df[filt]\n",
    "\n",
    "print(f'Row count dropped from {row_count} to {len(gifts_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of transactions is {len(gifts_df[Columns.INVOICE_NO].unique())}')\n",
    "print(f'Number of items is {len(gifts_df[Columns.DESCRIPTION].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover unique InvoiceNo's.\n",
    "invoice_numbers = gifts_df[Columns.INVOICE_NO].unique()\n",
    "print(f'{len(invoice_numbers)} unique invoice numbers was found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transactions(df):\n",
    "    #Create a basket of items for each transaction\n",
    "    transactions = []\n",
    "\n",
    "    i = 1\n",
    "    for invoice_no in invoice_numbers:\n",
    "        if i % 500 == 0:\n",
    "            print(f'{i} invoice numbers were processed')\n",
    "        \n",
    "        filt = gifts_df[Columns.INVOICE_NO] == invoice_no\n",
    "        transaction = list(gifts_df[filt].description.astype(str))\n",
    "        transactions.append(transaction)\n",
    "        i = i + 1\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9477877",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_map_file_path = '../datasets/outputs/market_basket/gits_item_map.csv'\n",
    "is_item_map_file_exists = os.path.isfile(item_map_file_path)\n",
    "is_item_map_file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_map_df = None\n",
    "\n",
    "if is_item_map_file_exists:\n",
    "    item_map_df = pd.read_csv(item_map_file_path)\n",
    "    print('item_map_df already exists and was loaded')\n",
    "else:\n",
    "    print('Extracting transactions can take a few minutes\\n')\n",
    "    transactions = extract_transactions(gifts_df)\n",
    "    \n",
    "    # Instantiate transaction encoder.\n",
    "    encoder = TransactionEncoder()\n",
    "\n",
    "    # One-hot encode transactions.\n",
    "    item_map = encoder.fit(transactions).transform(transactions)\n",
    "\n",
    "    # Use unique items as column headers.\n",
    "    item_map_df = pd.DataFrame(item_map, columns = encoder.columns_).drop('nan', axis=1)\n",
    "\n",
    "    item_map_df.to_csv(index=False)\n",
    "\n",
    "    filepath = Path(item_map_file_path)  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    item_map_df.to_csv(filepath)\n",
    "\n",
    "    print('\\nStored item_map_df as a file on disk')\n",
    "        \n",
    "# Print onehot header.\n",
    "item_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate class examples\n",
    "aggregate = Aggregate(item_map_df)\n",
    "\n",
    "bags = aggregate.apply('bag')\n",
    "boxes = aggregate.apply('box')\n",
    "candles = aggregate.apply('candle')\n",
    "\n",
    "print('Share of Bags: %.2f' % bags.mean())\n",
    "print('Share of Boxes: %.2f' % boxes.mean())\n",
    "print('Share of Candles: %.2f' % candles.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ff433",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_map_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "item_map_df.head()\n",
    "\n",
    "frequent_itemsets = apriori(item_map_df, min_support=0.05, max_len = 3, use_colnames = True)\n",
    "frequent_itemsets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639b54a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting several apriori and association rules configurations. \n",
    "#In a such way, apriori running engine can be tested\n",
    "\n",
    "configuration = AprioriConfiguration()\n",
    "\n",
    "option = AprioriOption(min_support=0.04, max_len=3)\n",
    "option.association_rule = RAOption(metric=MetricType.SUPPORT, min_threshold=0.001)\n",
    "configuration.options.append(option)\n",
    "\n",
    "option = AprioriOption(min_support=0.05, max_len=3)\n",
    "option.association_rule = RAOption(metric=MetricType.SUPPORT, min_threshold=0.002)\n",
    "configuration.options.append(option)\n",
    "\n",
    "option = AprioriOption(min_support=0.03, max_len=2)\n",
    "option.association_rule = RAOption(metric=MetricType.LIFT, min_threshold=1.0)\n",
    "configuration.options.append(option)\n",
    "\n",
    "filter_option = FilterOption()\n",
    "filter_option.antecedent_support = 0.05\n",
    "filter_option.consequent_support = 0.05\n",
    "ar_option = RAOption(metric=MetricType.CONFIDENCE, min_threshold=0.4)\n",
    "\n",
    "option = AprioriOption(min_support=0.03, max_len=2)\n",
    "option.association_rule = ar_option\n",
    "configuration.options.append(option)\n",
    "\n",
    "option = AprioriOption(min_support=0.03, max_len=2)\n",
    "option.association_rule = ar_option\n",
    "option.filter = filter_option\n",
    "configuration.options.append(option)\n",
    "\n",
    "#Setting a configuration for the best run\n",
    "#Including filtering and a lift metric as a initial metric\n",
    "filter_option = FilterOption()\n",
    "filter_option.zhang = 0.95\n",
    "filter_option.antecedent_support = 0.05\n",
    "filter_option.consequent_support = 0.06\n",
    "ar_option = RAOption(metric=MetricType.LIFT, min_threshold=1.00)\n",
    "\n",
    "option = AprioriOption(min_support=0.03, max_len=2)\n",
    "option.association_rule = ar_option\n",
    "option.filter = filter_option\n",
    "configuration.options.append(option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22a03f",
   "metadata": {},
   "source": [
    "## Algorithm run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_alg = Apriori(configuration, item_map_df)\n",
    "results = apriori_alg.run()\n",
    "\n",
    "statistics = results.statistics.get().sort_values(by=['rules'])\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since both support and confidence metrics are less strong than a lift we will discard theirs results\n",
    "lift_statistics = statistics[statistics['rule_metric'] == MetricType.LIFT]\n",
    "lift_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting best result from the lift results\n",
    "best_result = lift_statistics.head(1)\n",
    "best_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a07ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the best result rule set\n",
    "best_result_index = best_result.index.values[0]\n",
    "results.items[best_result_index].rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_configuration = configuration.get_option(best_result_index)\n",
    "best_configuration.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
