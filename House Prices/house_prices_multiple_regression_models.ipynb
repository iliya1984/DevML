{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036c5ceb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63016b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(np.version.version)\n",
    "\n",
    "import seaborn as sns\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from feature_engine.encoding import RareLabelEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "from torcheval.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331d3dc",
   "metadata": {},
   "source": [
    "## Constant Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCategory:\n",
    "    CATEGORICAL = 'CATEGORICAL'\n",
    "    NUMERICAL = 'NUMERICAL'\n",
    "    ORDINAL = 'ORDINAL',\n",
    "    ALL = 'ALL'\n",
    "\n",
    "target_name = 'SalePrice'\n",
    "    \n",
    "num_features = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', \\\n",
    "                'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \\\n",
    "                 '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', \\\n",
    "                 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', \\\n",
    "                 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', \\\n",
    "                 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \\\n",
    "                 'MiscVal', 'MoSold', 'YrSold']\n",
    "\n",
    "cat_features = ['MSZoning', 'Street', 'Alley', \\\n",
    "                'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', \\\n",
    "                'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \\\n",
    "                'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', \\\n",
    "                'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', \\\n",
    "                'BsmtExposure','BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', \\\n",
    "                'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', \\\n",
    "                'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', \\\n",
    "                'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition' ]\n",
    "\n",
    "ord_features = ['MSSubClass', 'OverallQual', 'OverallCond' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18e94b",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd92965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAnalysis:\n",
    "    \n",
    "    def assign_df(self, df, df_to_append):\n",
    "        for column in df_to_append.columns:\n",
    "            df[column] = df_to_append[column].values\n",
    "    \n",
    "    def has_features_to_impute(self, df, features, feature_category, is_test):\n",
    "        features_to_impute = self.analyse_missing_features(df, features, True)\n",
    "        \n",
    "        if (is_test):\n",
    "            set_type = 'test'\n",
    "        else:\n",
    "            set_type = 'train'\n",
    "        \n",
    "        if(len(features_to_impute) == 0):\n",
    "            print(f'All {set_type} {feature_category} features were imputed successfully')\n",
    "            return False\n",
    "        else:\n",
    "            print(f'Warning ! Not all {feature_category} features were imputed')\n",
    "            return True\n",
    "    \n",
    "    def validate_imputation(self, x_train, x_test, features):\n",
    "        features_to_impute = self.analyse_missing_features(x_train, features, True)\n",
    "        features_no_impute = self.analyse_missing_features(x_train, features, False)\n",
    "\n",
    "        test_features_to_impute = self.analyse_missing_features(x_test, features, True)\n",
    "        test_features_no_impute = self.analyse_missing_features(x_test, features, False)\n",
    "        \n",
    "        print(f'Number of train features to impute = {len(features_to_impute)}')\n",
    "        print(f'Number of train features without missing values = {len(features_no_impute)}')\n",
    "        print(f'Number of test features to impute = {len(test_features_to_impute)}')\n",
    "        print(f'Number of test features without missing values = {len(test_features_no_impute)}')\n",
    "        \n",
    "        if (not len(features_to_impute) == len(test_features_to_impute)):\n",
    "            print(f'Train and test feature number to impute is NOT the same')\n",
    "            return False\n",
    "        \n",
    "       \n",
    "        if (not features_to_impute.index.equals(test_features_to_impute.index)):\n",
    "            print(f'Train and test features to impute are NOT the same')\n",
    "            return False\n",
    "        \n",
    "        print('\\n')\n",
    "        return True\n",
    "        \n",
    "    \n",
    "    def analyse_missing_features(self, df, feature_list, to_impute):\n",
    "        if (to_impute):\n",
    "            return df[feature_list] \\\n",
    "                    .isnull().mean() \\\n",
    "                    .loc[lambda x : x > 0] \\\n",
    "                    .sort_values(ascending=False)\n",
    "        else:\n",
    "            return df[feature_list] \\\n",
    "                    .isnull().mean() \\\n",
    "                    .loc[lambda x : x == 0] \\\n",
    "                    .sort_values(ascending=False)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def unique_values(self, df, feature_name):\n",
    "        feature_values = df[feature_name].unique()\n",
    "        feature_values.sort()\n",
    "        return feature_values\n",
    "    \n",
    "    def draw_feature_plots(self, df, feature_name, target_name, is_categorical):\n",
    "        feature = df[feature_name]\n",
    "        fig, axs = plt.subplots(2, 2)\n",
    "        fig.suptitle(f'\"{feature_name}\" feature analysis')\n",
    "\n",
    "        axs[0, 0].hist(feature)\n",
    "        \n",
    "        if (is_categorical == False):\n",
    "            axs[0, 1].boxplot(feature)\n",
    "    \n",
    "        stats.probplot(df[feature_name], dist='norm', plot=axs[1, 0])\n",
    "        \n",
    "        if (not (target_name == '')):\n",
    "            axs[1, 1].scatter(feature, df[target_name])\n",
    "\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0cb34",
   "metadata": {},
   "source": [
    "## Loading training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../datasets/house_prices/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0caee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_df, train_df[target_name], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of FEATURES = ' + str(len(num_features) + len(cat_features) + len(ord_features)))\n",
    "print('Number of ROWS = ' + str(len(train_df)))\n",
    "print('Number of the NUMERICAL features = ' + str(len(num_features)))\n",
    "print('Number of the CATEGORICAL features = ' + str(len(cat_features)))\n",
    "print('Number of the ORDINAL features = ' + str(len(ord_features)))\n",
    "print(f'Train X dataset size = {len(x_train)}')\n",
    "print(f'Test X dataset size = {len(x_test)}')\n",
    "print('All FEATURES = ' + str(x_train.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f16471",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FeatureAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f604ad6",
   "metadata": {},
   "source": [
    "## Feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e898b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_features = []\n",
    "imputed_train_df = pd.DataFrame()\n",
    "imputed_test_df = pd.DataFrame()\n",
    "median_imputer = SimpleImputer(missing_values=np.NaN, strategy='median')\n",
    "frequent_cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974e339",
   "metadata": {},
   "source": [
    "### Numerical feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da37bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_to_impute = fa.analyse_missing_features(x_train, num_features, True)\n",
    "num_features_no_impute = fa.analyse_missing_features(x_train, num_features, False)\n",
    "\n",
    "is_valid = fa.validate_imputation(x_train, x_test, num_features)\n",
    "\n",
    "if(is_valid):\n",
    "    print('Numberical feature imputation is VALID')\n",
    "else:\n",
    "    print('Numerical feature imputaion ERROR')\n",
    "\n",
    "fa.assign_df(imputed_train_df, x_train[num_features_no_impute.index])\n",
    "fa.assign_df(imputed_test_df, x_test[num_features_no_impute.index])\n",
    "\n",
    "imputed_features = imputed_features + num_features_no_impute.index.values.tolist()\n",
    "\n",
    "print(f'Number of imputed features is: {len(imputed_features)}')\n",
    "num_features_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imputer.fit(x_train[num_features_to_impute.index])\n",
    "\n",
    "num_features_imputed = median_imputer.transform(x_train[num_features_to_impute.index])\n",
    "test_num_features_imputed = median_imputer.transform(x_test[num_features_to_impute.index])\n",
    "\n",
    "num_features_imputed_df = pd.DataFrame(num_features_imputed, columns=num_features_to_impute.index)\n",
    "test_num_features_imputed_df = pd.DataFrame(test_num_features_imputed, columns=num_features_to_impute.index)\n",
    "\n",
    "fa.assign_df(imputed_train_df, num_features_imputed_df)\n",
    "fa.assign_df(imputed_test_df, test_num_features_imputed_df)\n",
    "\n",
    "imputed_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_features = imputed_features + num_features_to_impute.index.values.tolist()\n",
    "\n",
    "fa.has_features_to_impute(imputed_train_df, num_features, FeatureCategory.NUMERICAL, False)\n",
    "fa.has_features_to_impute(imputed_test_df, num_features, FeatureCategory.NUMERICAL, True)\n",
    "    \n",
    "print(f'Number of impute features is: {len(imputed_features)}')\n",
    "\n",
    "imputed_train_df.isnull().mean()[lambda x : x > 0]\n",
    "len(imputed_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e7009",
   "metadata": {},
   "source": [
    "### Categorical feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe98c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_no_impute = fa.analyse_missing_features(x_train, cat_features, False)\n",
    "cat_features_no_impute.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbede6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing all categorical features to impute\n",
    "cat_features_to_impute = fa.analyse_missing_features(x_train, cat_features, True)\n",
    "cat_features_no_impute = fa.analyse_missing_features(x_train, cat_features, False)\n",
    "\n",
    "is_valid = fa.validate_imputation(x_train, x_test, cat_features)\n",
    "\n",
    "if(is_valid):\n",
    "    print('Categorical feature imputation is VALID')\n",
    "else:\n",
    "    print('Categorical feature imputaion ERROR')\n",
    "\n",
    "fa.assign_df(imputed_train_df, x_train[cat_features_no_impute.index])\n",
    "fa.assign_df(imputed_test_df, x_test[cat_features_no_impute.index])\n",
    "\n",
    "cat_features_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e162992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping features such that missing value percent is greater than 20%\n",
    "not_imputable_cat_features = cat_features_to_impute.loc[lambda x : x >= 0.1]\n",
    "cat_features_to_impute = cat_features_to_impute.loc[lambda x : x < 0.1]\n",
    "\n",
    "print(f'Number of the not imputable feature is {len(not_imputable_cat_features)}')\n",
    "\n",
    "cat_features_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d559d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in cat_features_to_impute.index:\n",
    "    most_frequent_category = x_train[feature_name].mode()\n",
    "    \n",
    "    imputed_train_df[feature_name] = x_train[feature_name].values\n",
    "    imputed_train_df.loc[imputed_train_df[feature_name].isnull(), feature_name] = most_frequent_category[0]\n",
    "    \n",
    "    imputed_test_df[feature_name] = x_test[feature_name].values\n",
    "    imputed_test_df.loc[imputed_test_df[feature_name].isnull(), feature_name] = most_frequent_category[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a935b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Listing not imputable categorical features')\n",
    "not_imputable_cat_feature_list = not_imputable_cat_features.index.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_cat_features = []\n",
    "\n",
    "for feature_name in cat_features:\n",
    "    if(not (feature_name in not_imputable_cat_feature_list)):\n",
    "        imputed_cat_features.append(feature_name)\n",
    "\n",
    "    \n",
    "imputed_features = imputed_features + imputed_cat_features\n",
    "print(f'Number of impute features is: {len(imputed_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa.has_features_to_impute(imputed_train_df, imputed_cat_features, FeatureCategory.CATEGORICAL, False)\n",
    "fa.has_features_to_impute(imputed_test_df, imputed_cat_features, FeatureCategory.CATEGORICAL, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201de874",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0026e60",
   "metadata": {},
   "source": [
    "### Ordianl feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_features_to_impute = fa.analyse_missing_features(x_train, ord_features, True)\n",
    "ord_features_no_impute = fa.analyse_missing_features(x_train, ord_features, False)\n",
    "\n",
    "if (len(ord_features_to_impute) == 0):\n",
    "    print('No missing values were found for the ordinal features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa.has_features_to_impute(imputed_train_df, imputed_train_df.columns, FeatureCategory.ALL, False)\n",
    "fa.has_features_to_impute(imputed_test_df, imputed_test_df.columns, FeatureCategory.ALL, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal features do not have missing values\n",
    "# Adding all ordinal features to the imputed dataframe\n",
    "\n",
    "fa.assign_df(imputed_train_df, x_train[ord_features_no_impute.index])\n",
    "fa.assign_df(imputed_test_df, x_test[ord_features_no_impute.index])\n",
    "    \n",
    "imputed_features = imputed_features + ord_features_no_impute.index.values.tolist()\n",
    "print(f'Number of impute features is: {len(imputed_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c78e",
   "metadata": {},
   "source": [
    "## Categorical feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7aa14a",
   "metadata": {},
   "source": [
    "### Rare categories handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ef7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in imputed_cat_features:\n",
    "    feature_cat_info = imputed_train_df.groupby(feature_name)[feature_name].count() / len(imputed_train_df)\n",
    "    feature_cat_info = feature_cat_info.sort_values(ascending=False)\n",
    "    \n",
    "    all_cat_number = len(feature_cat_info)\n",
    "    rare_cat_number = len(feature_cat_info.loc[lambda x : x > 0.05])\n",
    "    non_rare_cat_number = all_cat_number - rare_cat_number\n",
    "\n",
    "    print(f'Feature \"{feature_name}\" has {rare_cat_number} rare categories and {non_rare_cat_number} NON rare') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_label_encoder = RareLabelEncoder(tol=0.05, n_categories=2, variables=imputed_cat_features, replace_with='Rare')\n",
    "rare_label_encoder.fit(imputed_train_df[imputed_cat_features])\n",
    "\n",
    "rare_cat_encoded_df = rare_label_encoder.transform(imputed_train_df[imputed_cat_features])\n",
    "test_rare_cat_encoded_df = rare_label_encoder.transform(imputed_test_df[imputed_cat_features])\n",
    "\n",
    "fa.assign_df(imputed_train_df, rare_cat_encoded_df[imputed_cat_features])\n",
    "fa.assign_df(imputed_test_df, test_rare_cat_encoded_df[imputed_cat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba6772",
   "metadata": {},
   "source": [
    "### Categorical feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OrdinalEncoder(encoding_method='ordered', variables=imputed_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index = imputed_train_df.index\n",
    "\n",
    "cat_encoder.fit(imputed_train_df, y_train)\n",
    "\n",
    "imputed_train_df = cat_encoder.transform(imputed_train_df)\n",
    "imputed_test_df = cat_encoder.transform(imputed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5847cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train_df[imputed_cat_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf2a44",
   "metadata": {},
   "source": [
    "## Feature Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85525ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(imputed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94adc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scalling maximum values.')\n",
    "scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scalling minimum values.')\n",
    "scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalled_train_df = pd.DataFrame(scaler.transform(imputed_train_df), columns=imputed_train_df.columns)\n",
    "scalled_test_df = pd.DataFrame(scaler.transform(imputed_test_df), columns=imputed_test_df.columns)\n",
    "\n",
    "scalled_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50352a",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50997d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(estimator=LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d006664",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index = scalled_train_df.index\n",
    "\n",
    "selector.fit(scalled_train_df, y_train)\n",
    "selected_features_index = scalled_train_df.columns[selector.get_support()]\n",
    "selected_features = selected_features_index.values\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe88f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalled_train_df[selected_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651ef9e",
   "metadata": {},
   "source": [
    "## Linear Regression model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "regressor.fit(scalled_train_df[selected_features], y_train)\n",
    "predicated_y_test = regressor.predict(scalled_test_df[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The coefficients\n",
    "print('Coefficients: \\n', regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14471f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mean squered error\n",
    "print('The mean squered error is:')\n",
    "mean_squared_error(y_test, predicated_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The R2 score\n",
    "print('The R squered score is:')\n",
    "r2_score(y_test, predicated_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c2376",
   "metadata": {},
   "source": [
    "## Neural Network model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db59a9f",
   "metadata": {},
   "source": [
    "### Network configuration classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9720ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerType:\n",
    "    INPUT = 'Input'\n",
    "    OUTPUT = 'Output'\n",
    "    HIDDEN = 'Hiddent'\n",
    "\n",
    "class NetConfiguration:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = 0\n",
    "        self.epochs = 0\n",
    "        self.batch_size = 0\n",
    "    \n",
    "    \n",
    "class LayerItem:\n",
    "    \n",
    "    def __init__(self, name, layer_type, input_size, output_size):\n",
    "        \n",
    "        self.name = name\n",
    "        self.type = layer_type\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Layer({self.name})={self.type}({self.input_size}, {self.output_size})'\n",
    "        \n",
    "class LayerConfiguration:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.__layers = []\n",
    "        self.__hidden_layers = []\n",
    "        self.__input_layer = None\n",
    "        self.__output_layer = None\n",
    "\n",
    "    def __add_and_get(self, layer_name, layer_type, input_size, output_size):\n",
    "        \n",
    "        layer = LayerItem(layer_name, layer_type, input_size, output_size)\n",
    "        self.__layers.append(layer)\n",
    "        return layer\n",
    "        \n",
    "    def use_input_layer(self, input_size, output_size):\n",
    "        \n",
    "        layer_name = 'input'\n",
    "        layer_type = LayerType.INPUT\n",
    "        layer = self.__add_and_get(layer_name, layer_type, input_size, output_size)\n",
    "        self.__input_layer = layer\n",
    "        return self\n",
    "        \n",
    "    def use_output_layer(self, input_size, output_size):\n",
    "        \n",
    "        layer_name = 'output'\n",
    "        layer_type = LayerType.OUTPUT\n",
    "        layer = self.__add_and_get(layer_name, layer_type, input_size, output_size)\n",
    "        self.__output_layer = layer\n",
    "        return self\n",
    "    \n",
    "    def use_hidden_layer(self, input_size, output_size):\n",
    "        \n",
    "        layer_id = len(self.__hidden_layers) + 1\n",
    "        \n",
    "        layer_name = f'hidden_{layer_id}'\n",
    "        layer_type = LayerType.HIDDEN\n",
    "        layer = self.__add_and_get(layer_name, layer_type, input_size, output_size)\n",
    "        self.__hidden_layers.append(layer)\n",
    "        return self\n",
    "\n",
    "    def use_hidden_layers(self, size, units):\n",
    "        \n",
    "        for unit in range(units):\n",
    "            self.use_hidden_layer(size, size)\n",
    "\n",
    "        return self\n",
    "            \n",
    "    def get_input_layer(self):\n",
    "        return self.__input_layer\n",
    "    \n",
    "    def get_output_layer(self):\n",
    "        return self.__output_layer\n",
    "    \n",
    "    def get_hidden_layers(self):\n",
    "        return self.__hidden_layers\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \n",
    "        result = ''\n",
    "        for layer in self.__layers:\n",
    "            result = f'{result} \\n{str(layer)}'\n",
    "            \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285c5cf",
   "metadata": {},
   "source": [
    "### Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = len(scalled_train_df.columns)\n",
    "hidden_layer_size = 64\n",
    "hidden_layers = 2\n",
    "\n",
    "layer_configuration = LayerConfiguration()\n",
    "layer_configuration \\\n",
    "    .use_input_layer(input_layer_size, hidden_layer_size) \\\n",
    "    .use_hidden_layers(hidden_layer_size, hidden_layers) \\\n",
    "    .use_output_layer(hidden_layer_size, 1)\n",
    "\n",
    "net_config = NetConfiguration(layer_configuration)\n",
    "net_config.learning_rate = .001\n",
    "net_config.epochs = 100\n",
    "net_config.batch_size = 64\n",
    "\n",
    "\n",
    "print(str(layer_configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePricesExperiment:\n",
    "    \n",
    "    def __init__(self, config, model, loss_method, optimizer):\n",
    "        self.epochs = config.epochs\n",
    "        self.batch_size = config.batch_size\n",
    "        \n",
    "        #Initialing ANN\n",
    "        self.model = model\n",
    "\n",
    "        #Initializing loss function\n",
    "        self.loss_method = loss_method\n",
    "\n",
    "        #Initialing optimizer\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def train(self, train_df, targets):\n",
    "        \n",
    "        targets_df = pd.DataFrame(targets.tolist(), columns=[target_name])\n",
    "        \n",
    "        train_tensor = torch.tensor(train_df.values).float()\n",
    "        targets_tensor = torch.tensor(targets_df.values).float()\n",
    "        \n",
    "        print(f'Features tensor size is {train_tensor.size()}')\n",
    "        print(f'Targets tensor size is {targets_tensor.size()}\\n')\n",
    "        \n",
    "        train_dataset = TensorDataset(train_tensor,targets_tensor)\n",
    "        batchsize    = self.batch_size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "        \n",
    "        losses = torch.zeros(self.epochs)\n",
    "        for epochi in range(self.epochs):\n",
    "           \n",
    "            batchAcc  = []\n",
    "            batchLoss = []\n",
    "            \n",
    "            # loop over training data batches        \n",
    "            for X, y in train_loader:\n",
    "                \n",
    "                #Forward step\n",
    "                predictions = self.model(X)\n",
    "            \n",
    "                #Calculation loss\n",
    "                loss = self.loss_method(predictions, y)\n",
    "                batchLoss.append(loss.item())\n",
    "            \n",
    "                #Backward step\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "             \n",
    "            losses[epochi] = np.mean(batchLoss)\n",
    "        predictions = self.model(train_tensor)\n",
    "        \n",
    "        r2s_metric = R2Score()\n",
    "        r2s_metric.update(predictions, targets_tensor)\n",
    "        accuracy = r2s_metric.compute()\n",
    "        \n",
    "        return losses, predictions, accuracy\n",
    "\n",
    "class HousePricesNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        layer_config = config.layers.get_input_layer()        \n",
    "        self.input = nn.Linear(layer_config.input_size, layer_config.output_size)\n",
    "\n",
    "        self.hidden = nn.ModuleList()\n",
    "        \n",
    "        for layer_config in config.layers.get_hidden_layers():\n",
    "            self.hidden.append(nn.Linear(layer_config.input_size, layer_config.output_size))\n",
    "            \n",
    "        layer_config = config.layers.get_output_layer()        \n",
    "        self.output = nn.Linear(layer_config.input_size, layer_config.output_size)\n",
    "  \n",
    "    def forward(self, x_train):\n",
    "        \n",
    "        data = self.input(x_train)\n",
    "        data = F.relu(data)\n",
    "        \n",
    "        for hidden_layer in self.hidden:\n",
    "            data = hidden_layer(data)\n",
    "            data = F.relu(data)\n",
    "        \n",
    "        result = self.output(data)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HousePricesNet(net_config)\n",
    "loss_method = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=net_config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = HousePricesExperiment(net_config, model, loss_method, optimizer)\n",
    "\n",
    "losses, predictions, accuracy = experiment.train(scalled_train_df, y_train)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "print(f'Training accuracy is {accuracy}')\n",
    "plt.plot(losses.detach(), markerfacecolor='w', linewidth=2)\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
