{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eef222",
   "metadata": {},
   "source": [
    "# Training and testiong a ANN for the Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779f624",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e516a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaa418",
   "metadata": {},
   "source": [
    "## Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputationMethod:\n",
    "    FREQUENT_CATEGORY = 'FrequentCategory'\n",
    "    MISSING_CATEGORY = 'MissingCategory'\n",
    "    KNN = 'KNN'\n",
    "\n",
    "class TicketFeatures:\n",
    "    TICKET = 'Ticket'\n",
    "    TICKET_PARSED = 'TicketParsed'\n",
    "    TICKET_NUMBER = 'TicketNumber'\n",
    "    TICKET_NUMBER_BINNED = 'TicketNumberBinned'\n",
    "    TICKET_PREFIX = 'TicketPrefix'\n",
    "    TICKET_LABEL = 'ticket_label'\n",
    "\n",
    "class CabinFeatures:\n",
    "    CABIN = 'Cabin'\n",
    "    CABIN_PARSED = 'CabinParsed'\n",
    "    CABIN_NUMBER = 'CabinNumber'\n",
    "    CABIN_NUMBER_BINNED = 'CabinNumberBinned'\n",
    "    DECK_CODE = 'DeckCode'\n",
    "    CABIN_LABEL = 'cabin_label'\n",
    "\n",
    "class ImputationConstants:\n",
    "    CABIN_NUMBER_IMPUTATION_VALUE = 999\n",
    "    \n",
    "target_name = 'survived'\n",
    "    \n",
    "features = ['sex', 'age', 'sib_sp', 'par_ch','embarked', \\\n",
    "            TicketFeatures.TICKET_LABEL, CabinFeatures.CABIN_LABEL, 'deck_code']\n",
    "\n",
    "num_features = ['age']\n",
    "cat_features = ['sex', 'sib_sp', 'par_ch','embarked', TicketFeatures.TICKET_LABEL, \\\n",
    "                CabinFeatures.CABIN_LABEL, 'deck_code']\n",
    "\n",
    "features_to_encode = ['sex', 'embarked', 'deck_code']\n",
    "\n",
    "feature_to_scale =  ['age', 'sib_sp', 'par_ch' , TicketFeatures.TICKET_LABEL, CabinFeatures.CABIN_LABEL]\n",
    "\n",
    "binning_quantiles = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1]\n",
    "\n",
    "index_name = 'PassengerId'\n",
    "index_alias = 'pax_id'\n",
    "\n",
    "column_rename = { 'Survived' : 'survived', 'Pclass' : 'pax_class',  \\\n",
    "                'Sex' : 'sex', 'Age' : 'age', 'SibSp' : 'sib_sp', 'Parch' : 'par_ch', \\\n",
    "                'Embarked' : 'embarked', TicketFeatures.TICKET_NUMBER_BINNED : TicketFeatures.TICKET_LABEL, \\\n",
    "                 CabinFeatures.CABIN_NUMBER_BINNED : CabinFeatures.CABIN_LABEL, \\\n",
    "                'DeckCode' : 'deck_code'}\n",
    "\n",
    "ann_config = { \n",
    "    'hidden_layer_size' : 32, \n",
    "    'learning_rate' : .1,\n",
    "    'epochs' : 2000 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4c5c0",
   "metadata": {},
   "source": [
    "## Loading the Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.read_csv('../datasets/titanic/train.csv')\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d5577",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3a38f",
   "metadata": {},
   "source": [
    "Feature description:\n",
    "\n",
    " 1. Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file.\n",
    " 2. Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    " 3. Name - Name\n",
    " 4. Sex - Sex\n",
    " 5. Age - Age\n",
    " 6. Sibsp - Number of Siblings/Spouses Aboard\n",
    " 7. Parch - Number of Parents/Children Aboard\n",
    " 8. Ticket - Ticket Number\n",
    " 9. Fare - Passenger Fare\n",
    " 10. Cabin - Cabin\n",
    " 11. Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbce765",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cabin:\n",
    "    def __init__(self, deck_code, number):\n",
    "        self.__deck_code = deck_code\n",
    "        self.__number = number\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Cabin no. {self.__number}, deck code is \"{self.__deck_code}\"'\n",
    "        \n",
    "    @property    \n",
    "    def deck_code(self):\n",
    "        return self.__deck_code\n",
    "    \n",
    "    @deck_code.setter\n",
    "    def deck_code(self, value):\n",
    "        self.__deck_code = value\n",
    "        \n",
    "    @property    \n",
    "    def number(self):\n",
    "        return self.__number\n",
    "    \n",
    "    @number.setter\n",
    "    def number(self, value):\n",
    "        self.__number = value\n",
    " \n",
    "    def parse(cabin):\n",
    "        try:\n",
    "            assert not (cabin is np.nan) \n",
    "            cabin_items = cabin.split(\" \")\n",
    "            \n",
    "            parsed_cabins = []\n",
    "            for item in cabin_items:\n",
    "                parsed_cabin = Cabin.parse_item(item)\n",
    "                parsed_cabins.append(parsed_cabin)\n",
    "            \n",
    "            return parsed_cabins\n",
    "        except ValueError as e:\n",
    "            print(f'Unable parse the \"{cabin}\" cabin. Details: {e}')\n",
    "\n",
    "    def parse_item(cabin):\n",
    "        try:\n",
    "            assert not (cabin is np.nan)\n",
    "            \n",
    "            cabin_number = cabin[1 :]\n",
    "            deck_code = cabin[0]\n",
    "            \n",
    "            return Cabin(deck_code, cabin_number)\n",
    "        except ValueError as e:\n",
    "            print(f'Unable parse the \"{cabin}\" cabin ITEM. Details: {e}')\n",
    "\n",
    "class Ticket:\n",
    "    def __init__(self, prefix, number):\n",
    "        self.__prefix = prefix\n",
    "        self.__number = number\n",
    "\n",
    "    def __str__(self):\n",
    "        \n",
    "        ticket = ''\n",
    "        if(not self.__prefix is np.nan):\n",
    "            ticket = self.__prefix\n",
    "        \n",
    "        return f'Ticket {ticket} {self.__number} no. {self.__number}'\n",
    "        \n",
    "    @property    \n",
    "    def prefix(self):\n",
    "        return self.__prefix\n",
    "    \n",
    "    @prefix.setter\n",
    "    def prefix(self, value):\n",
    "        self.__prefix = value\n",
    "        \n",
    "    @property    \n",
    "    def number(self):\n",
    "        return self.__number\n",
    "    \n",
    "    @number.setter\n",
    "    def number(self, value):\n",
    "        self.__number = value\n",
    "            \n",
    "    def parse(ticket):\n",
    "        try:\n",
    "            ticket_number = np.nan\n",
    "            ticket_prefix = np.nan\n",
    "            ticket_splited = ticket.split(\" \")\n",
    "    \n",
    "            if (ticket == 'LINE'):\n",
    "                ticket_prefix = ticket\n",
    "            elif (len(ticket_splited) == 1):\n",
    "                ticket_number = int(ticket_splited[0])\n",
    "            elif (len(ticket_splited) == 2):\n",
    "                ticket_number = int(ticket_splited[1])\n",
    "                ticket_prefix = str(ticket_splited[0])\n",
    "            elif (len(ticket_splited) == 3):\n",
    "                ticket_number = int(ticket_splited[2])\n",
    "                ticket_prefix = f'{ticket_splited[0]} {ticket_splited[1]}'\n",
    "            else:\n",
    "                raise Exception(f'Unsupported ticket format.')\n",
    "                \n",
    "            return Ticket(ticket_prefix, ticket_number)\n",
    "        except ValueError as e:\n",
    "            print(f'Unable parse the \"{ticket}\" ticket. Details: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dcfd8e",
   "metadata": {},
   "source": [
    "## Untility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc769fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImputer:\n",
    "     \n",
    "    def __init__(self):\n",
    "        self.__instances = {\n",
    "            ImputationMethod.FREQUENT_CATEGORY : CategoricalImputer(imputation_method='frequent'),\n",
    "            ImputationMethod.MISSING_CATEGORY : CategoricalImputer(imputation_method='missing'),\n",
    "            ImputationMethod.KNN : KNNImputer(n_neighbors=5, weights='distance', metric='nan_euclidean')\n",
    "        }\n",
    "    \n",
    "    def transform(self, imputation_method, df):\n",
    "        imputer = self.__instances[imputation_method];\n",
    "        return imputer.transform(df)\n",
    "    \n",
    "    def fit_transform(self, imputation_method, x_train, x_test, variables):\n",
    "        imputer = self.__instances[imputation_method];\n",
    "        imputer.fit(x_train.loc[:, variables])\n",
    "        \n",
    "        x_train_imputed = x_train.copy()       \n",
    "        train_result = imputer.transform(x_train.loc[:, variables])\n",
    "        \n",
    "        x_test_imputed = x_test.copy()\n",
    "        test_result =  imputer.transform(x_test.loc[:, variables])\n",
    "        \n",
    "        if(imputation_method == ImputationMethod.KNN):\n",
    "            temp_df = pd.DataFrame(train_result, columns=variables)\n",
    "            temp_df.index = x_train_imputed.index\n",
    "            x_train_imputed.loc[:, variables] = temp_df[variables]\n",
    "            \n",
    "            temp_df = pd.DataFrame(test_result, columns=variables)\n",
    "            temp_df.index = x_test_imputed.index\n",
    "            x_test_imputed.loc[:, variables] = temp_df[variables]\n",
    "        else:\n",
    "            x_train_imputed.loc[:, variables] = train_result\n",
    "            x_test_imputed.loc[:, variables] = test_result\n",
    "            \n",
    "        return x_train_imputed, x_test_imputed\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f14d4",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fe615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set PassengerId as an index\n",
    "titanic_df.set_index(index_name, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67177598",
   "metadata": {},
   "source": [
    "### Ticket feature preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_ticket_filter = titanic_df[TicketFeatures.TICKET].isnull()\n",
    "null_tickets_number = len(titanic_df[TicketFeatures.TICKET].loc[null_ticket_filter])\n",
    "\n",
    "print(f'NULL tickets number is: {null_tickets_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4362293",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df[TicketFeatures.TICKET_PARSED] = titanic_df[TicketFeatures.TICKET].apply(lambda t : Ticket.parse(t))\n",
    "titanic_df[TicketFeatures.TICKET_PARSED].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d10ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Ticket_Number feature\n",
    "titanic_df[TicketFeatures.TICKET_NUMBER] =  titanic_df[TicketFeatures.TICKET_PARSED].apply(lambda t : t.number)\n",
    "titanic_df[TicketFeatures.TICKET_NUMBER].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ce39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Ticket_Prefix feature\n",
    "titanic_df[TicketFeatures.TICKET_PREFIX] =  titanic_df[TicketFeatures.TICKET_PARSED].apply(lambda t : t.prefix)\n",
    "titanic_df[TicketFeatures.TICKET_PREFIX].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing ticket feature statisitcs\n",
    "filt = titanic_df[TicketFeatures.TICKET_PREFIX].notnull()\n",
    "not_null_ticket_prefixes_number = len(titanic_df.loc[filt])\n",
    "\n",
    "filt = titanic_df[TicketFeatures.TICKET_NUMBER].notnull()\n",
    "not_null_ticket_numbers_number = len(titanic_df.loc[filt])\n",
    "\n",
    "print(f'Not NULL ticket prefixes number is: {not_null_ticket_prefixes_number}')\n",
    "print(f'NULL ticket prefixes number is: {len(titanic_df) - not_null_ticket_prefixes_number}')\n",
    "print(f'Not NULL ticket numbers number is: {not_null_ticket_numbers_number}')\n",
    "print(f'NULL ticket numbers number is: {len(titanic_df) - not_null_ticket_numbers_number}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda9f13",
   "metadata": {},
   "source": [
    "### Cabin feature preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cabin_filter = titanic_df[CabinFeatures.CABIN].isnull()\n",
    "null_cabins_number = len(titanic_df[CabinFeatures.CABIN].loc[null_cabin_filter])\n",
    "\n",
    "print(f'NULL cabins number is: {null_cabins_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c101088",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_cabin_filter = titanic_df[CabinFeatures.CABIN].notnull()\n",
    "titanic_df[CabinFeatures.CABIN_PARSED] = titanic_df.loc[not_null_cabin_filter, CabinFeatures.CABIN] \\\n",
    "    .apply(lambda c : Cabin.parse(c))\n",
    "\n",
    "titanic_df[CabinFeatures.CABIN_PARSED].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_parsed_not_null_filter = titanic_df[CabinFeatures.CABIN_PARSED].notnull()\n",
    "cabin_parsed_not_null = titanic_df.loc[cabin_parsed_not_null_filter, CabinFeatures.CABIN_PARSED] \\\n",
    "    .apply(lambda c : len(c)) \n",
    "\n",
    "cabin_statistics = cabin_parsed_not_null \\\n",
    "    .groupby(cabin_parsed_not_null.values) \\\n",
    "    .count()\n",
    "\n",
    "print('Cabin statistics\\n')\n",
    "print(cabin_statistics)\n",
    "\n",
    "singe_cabin_percent = round((cabin_statistics.at[1] / cabin_statistics.sum()) * 100, 2)\n",
    "singe_cabin_percent\n",
    "\n",
    "print(f'\\nSingle cabin percent = {singe_cabin_percent}%')\n",
    "if(singe_cabin_percent > 85 ):\n",
    "    print('The majority of passangers ordered a single cabin. Thus multiple cabins can be ignored. Only first cabin will be taken.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_parsed_cabin_filter = titanic_df[CabinFeatures.CABIN_PARSED].notnull()\n",
    "titanic_df[CabinFeatures.CABIN_NUMBER] = titanic_df.loc[null_parsed_cabin_filter, CabinFeatures.CABIN_PARSED] \\\n",
    "    .apply(lambda c : c[0].number)\n",
    "\n",
    "titanic_df[CabinFeatures.CABIN_NUMBER].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df[CabinFeatures.DECK_CODE] = titanic_df.loc[null_parsed_cabin_filter, CabinFeatures.CABIN_PARSED] \\\n",
    "    .apply(lambda c : c[0].deck_code)\n",
    "\n",
    "titanic_df[CabinFeatures.DECK_CODE].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arranging ticket numbers into 10 bins, assigning an interval mean for each bin value\n",
    "\n",
    "titanic_df[TicketFeatures.TICKET_NUMBER_BINNED] = pd \\\n",
    "    .qcut(titanic_df[TicketFeatures.TICKET_NUMBER], q=binning_quantiles) \\\n",
    "    .apply(lambda i : int((i.left + i.right) / 2))\n",
    "\n",
    "sorted_ticket_numbers = titanic_df[TicketFeatures.TICKET_NUMBER].sort_values().values\n",
    "sorted_binned_ticket_numbers = titanic_df[TicketFeatures.TICKET_NUMBER_BINNED].sort_values().values\n",
    "\n",
    "print(f'Arranging ticket numbers into {len(binning_quantiles)} bins, assigning an interval mean for each bin value')\n",
    "\n",
    "titanic_df[TicketFeatures.TICKET_NUMBER_BINNED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16921303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_number_to_int(n):\n",
    "    if (n is np.nan or n == ''):\n",
    "        return ImputationConstants.CABIN_NUMBER_IMPUTATION_VALUE\n",
    "    else:\n",
    "        return int(n)\n",
    "\n",
    "temp = titanic_df[CabinFeatures.CABIN_NUMBER].apply(cabin_number_to_int)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ab582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arranging cabin numbers into 10 bins, assigning an interval mean for each bin value\n",
    "titanic_df[CabinFeatures.CABIN_NUMBER_BINNED] = pd \\\n",
    "    .qcut(temp, q=binning_quantiles, duplicates='drop') \\\n",
    "    .apply(lambda i : int((i.left + i.right) / 2))\n",
    "\n",
    "sorted_cabin_numbers = temp.apply(lambda c : str(c)).sort_values().values\n",
    "sorted_binned_cabin_numbers = titanic_df[CabinFeatures.CABIN_NUMBER_BINNED].apply(lambda c : str(c)).sort_values().values\n",
    "\n",
    "print(f'Arranging cabin numbers into {len(binning_quantiles)} bins, assigning an interval mean for each bin value')\n",
    "\n",
    "titanic_df[CabinFeatures.CABIN_NUMBER_BINNED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db113646",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "ax[0, 0].plot(sorted_ticket_numbers)\n",
    "ax[0, 0].title.set_text('ORIGINAL ticket number sorted asc.')\n",
    "\n",
    "ax[0, 1].plot(sorted_binned_ticket_numbers)\n",
    "ax[0, 1].title.set_text('BINNED ticket number and sorted asc.')\n",
    "\n",
    "ax[1, 0].plot(sorted_cabin_numbers)\n",
    "ax[1, 0].title.set_text('ORIGINAL cabin number sorted asc.')\n",
    "\n",
    "ax[1, 1].plot(sorted_binned_cabin_numbers)\n",
    "ax[1, 1].title.set_text('BINNED cabin number and sorted asc.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.rename(columns=column_rename)\n",
    "titanic_df.index.name = index_alias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77930525",
   "metadata": {},
   "source": [
    "## Train and test dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    titanic_df[features], \n",
    "    titanic_df[target_name], \n",
    "    test_size=0.3, \n",
    "    random_state=123)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e1955",
   "metadata": {},
   "source": [
    "## Feature imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1c523",
   "metadata": {},
   "source": [
    "### Categorical feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b13dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_imputation_stats = x_train[cat_features].isnull().mean().sort_values(ascending=False)\n",
    "cat_features_simple_impute = cat_features_imputation_stats[lambda x : ((x > 0) & (x < 0.1))]\n",
    "cat_features_no_impute = cat_features_imputation_stats[lambda x : x == 0]\n",
    "cat_features_complex_impute = cat_features_imputation_stats[lambda x : x > 0.1]\n",
    "\n",
    "print(f'Simple imptation required for the {cat_features_simple_impute.index.values} features')\n",
    "print(f'Complex imptation required for the {cat_features_complex_impute.index.values} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24688030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical feature to impute,probability distribution analysis\n",
    "\n",
    "def plot_categorical_feature_hist(df, features):\n",
    "    fig, axis = plt.subplots(1, len(features))\n",
    "    fig.set_size_inches(15, 5)\n",
    "\n",
    "    i = 0\n",
    "    for feature_name in features:\n",
    "        data = df[feature_name].value_counts()\n",
    "\n",
    "        data.plot(ax=axis[i], kind='bar')\n",
    "        axis[i].title.set_text(f'{feature_name} feature histogram')\n",
    "        i = i + 1\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "cat_features_to_impute = cat_features_imputation_stats[lambda x : x > 0]\n",
    "plot_categorical_feature_hist(x_train, cat_features_to_impute.index.values)\n",
    "#Seems that all distributions are skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing categorical features with low missing value percent\n",
    "labels = cat_features_simple_impute.index.values\n",
    "\n",
    "imputer = FeatureImputer()\n",
    "\n",
    "imputation_method = ImputationMethod.FREQUENT_CATEGORY\n",
    "x_train_imputed, x_test_imputed = imputer.fit_transform(imputation_method, x_train, x_test, labels)\n",
    "\n",
    "x_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution after imputing categorical features with low missing value percent \n",
    "plot_categorical_feature_hist(x_train_imputed, cat_features_to_impute.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing categorical features with hight missing value percent\n",
    "labels = cat_features_complex_impute.index.values\n",
    "\n",
    "imputation_method = ImputationMethod.MISSING_CATEGORY\n",
    "x_train_imputed, x_test_imputed = imputer.fit_transform(imputation_method, x_train_imputed, x_test_imputed, labels)\n",
    "\n",
    "x_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a5228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution after imputing categorical features with high missing value percent \n",
    "plot_categorical_feature_hist(x_train_imputed, cat_features_to_impute.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009294d",
   "metadata": {},
   "source": [
    "## Numerical feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c87817",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_imputation_stats = x_train[num_features].isnull().mean().sort_values(ascending=False)\n",
    "num_features_impute = num_features_imputation_stats[lambda x : ((x > 0) & (x < 0.25))]\n",
    "num_features_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing numerical features with medium missing value percent\n",
    "labels = num_features_impute.index.values\n",
    "\n",
    "imputation_method = ImputationMethod.KNN\n",
    "x_train_imputed, x_test_imputed = imputer.fit_transform(imputation_method, x_train_imputed, x_test_imputed, labels)\n",
    "\n",
    "x_train_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacaa7ea",
   "metadata": {},
   "source": [
    "## Categorical feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(variables=features_to_encode, drop_last=True)\n",
    "\n",
    "encoder.fit(x_train_imputed)\n",
    "x_train_encoded = encoder.transform(x_train_imputed)\n",
    "x_test_encoded = encoder.transform(x_test_imputed)\n",
    "\n",
    "x_train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439737e",
   "metadata": {},
   "source": [
    "## Feature scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaller = MinMaxScaler()\n",
    "\n",
    "scaller.fit(x_train_encoded[feature_to_scale])\n",
    "\n",
    "\n",
    "x_train_scalled = x_train_encoded.copy()\n",
    "scalled_df = scaller.transform(x_train_encoded[feature_to_scale])\n",
    "x_train_scalled[feature_to_scale] = pd.DataFrame(scalled_df, columns=feature_to_scale).values\n",
    "\n",
    "x_test_scalled = x_test_encoded.copy()\n",
    "scalled_df = scaller.transform(x_test_encoded[feature_to_scale])\n",
    "x_test_scalled[feature_to_scale] = pd.DataFrame(scalled_df, columns=feature_to_scale).values\n",
    "\n",
    "x_train_scalled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9ad8a",
   "metadata": {},
   "source": [
    "## Training ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d14aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = len(x_train_scalled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a245831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicNetTraining:\n",
    "    \n",
    "    def __init__(self, config, model, loss_method, optimizer):\n",
    "        self.epochs = config['epochs']\n",
    "        \n",
    "        #Initialing ANN\n",
    "        self.model = model\n",
    "\n",
    "        #Initializing loss function\n",
    "        self.loss_method = loss_method\n",
    "\n",
    "        #Initialing SGD optimizer\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def execute(self, train_df, targets):\n",
    "        \n",
    "        targets_df = pd.DataFrame(targets.tolist(), columns=[target_name])\n",
    "        \n",
    "        train_tensor = torch.tensor(train_df.values).float()\n",
    "        targets_tensor = torch.tensor(targets_df.values).float()\n",
    "        \n",
    "        print(f'Features tensor size is {train_tensor.size()}')\n",
    "        print(f'Targets tensor size is {targets_tensor.size()}\\n')\n",
    "        \n",
    "        train_dataset = TensorDataset(train_tensor,targets_tensor)\n",
    "        batchsize    = 16\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "        \n",
    "        losses = torch.zeros(self.epochs)\n",
    "        for epochi in range(self.epochs):\n",
    "           \n",
    "            batchAcc  = []\n",
    "            batchLoss = []\n",
    "            \n",
    "            # loop over training data batches        \n",
    "            for X, y in train_loader:\n",
    "                \n",
    "                #Forward step\n",
    "                predictions = self.model(X)\n",
    "            \n",
    "                #Calculation loss\n",
    "                loss = self.loss_method(predictions, y)\n",
    "                batchLoss.append(loss.item())\n",
    "            \n",
    "                #Backward step\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "             \n",
    "            losses[epochi] = np.mean(batchLoss)\n",
    "        predictions = self.model(train_tensor)\n",
    "        accuracy = 100 * torch.mean(((predictions > .5) == targets_tensor).float())\n",
    "        \n",
    "        return losses, predictions, accuracy\n",
    "        \n",
    "\n",
    "class TitanicNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_layer_size = config['hidden_layer_size']\n",
    "        \n",
    "        #Input layer\n",
    "        self.input = nn.Linear(input_layer_size, hidden_layer_size)\n",
    "        \n",
    "        #Hidden layers\n",
    "        self.hidden = [nn.Linear(hidden_layer_size, hidden_layer_size)]\n",
    "        \n",
    "        #Output layer\n",
    "        self.output = nn.Linear(hidden_layer_size, 1)\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        \n",
    "        data = self.input(x_train)\n",
    "        data = F.relu(data)\n",
    "        \n",
    "        for hidden_layer in self.hidden:\n",
    "            data = hidden_layer(data)\n",
    "            data = F.relu(data)\n",
    "        \n",
    "        data = self.output(data)\n",
    "        result = torch.sigmoid(data)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TitanicNet(ann_config)\n",
    "loss_method = nn.BCELoss() #Alternativly use BCEWithLogitsLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ann_config['learning_rate'])\n",
    "\n",
    "model_training = TitanicNetTraining(ann_config, model, loss_method, optimizer)\n",
    "\n",
    "losses, predictions, accuracy = model_training.execute(x_train_scalled, y_train)\n",
    "\n",
    "print(f'Training accuracy is {accuracy}')\n",
    "plt.plot(losses.detach(), markerfacecolor='w', linewidth=2)\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af0c9a",
   "metadata": {},
   "source": [
    "## Testing ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc282988",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = pd.DataFrame(y_test.tolist(), columns=[target_name])\n",
    "test_tensor = torch.tensor(x_test_scalled.values).float()\n",
    "targets_tensor = torch.tensor(targets_df.values).float()\n",
    "\n",
    "predictions = model(test_tensor)\n",
    "test_accuracy = 100 * torch.mean(((predictions > .5) == targets_tensor).float())\n",
    "\n",
    "print(f'Testing accuracy is {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c3425",
   "metadata": {},
   "source": [
    "## ANN architecture change experents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b5273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
